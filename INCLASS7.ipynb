{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSTLF2BX6jH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11Ee3GJmywu",
        "outputId": "c89a0077-5c20-425e-868f-266d4b67065c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=a14fe01130a462c5bb24e31b73fb457b679ecf8a6004a80abe224e66f011fad7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: h11, wikipedia, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import wikipedia"
      ],
      "metadata": {
        "id": "Q2A8TGhKm3i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.) Set up OpenAI and the enviornment\n"
      ],
      "metadata": {
        "id": "7E9HEMJSX-3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apikey = \"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\""
      ],
      "metadata": {
        "id": "4zwwdkZDYDZN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client= openai.OpenAI(\n",
        "    api_key=apikey\n",
        ")"
      ],
      "metadata": {
        "id": "8IiKS0snlpYP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSL--dhXMvnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
      ],
      "metadata": {
        "id": "tOXc5_BTm9HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles= [\"Artificial\", \"Intelligent\", \"UCLA\"]"
      ],
      "metadata": {
        "id": "jFEF-h6tNJgh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_title =page_titles[0]"
      ],
      "metadata": {
        "id": "ZEBlMc-VNZmk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_result =wikipedia.search(page_title)"
      ],
      "metadata": {
        "id": "upFPbkr3Nd7D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = wikipedia.search(search_result[0])"
      ],
      "metadata": {
        "id": "nz0bYaIsNuqP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_content(page_title):\n",
        "  search_result =wikipedia.search(page_title)\n",
        "  page = wikipedia.page(search_result[0])\n",
        "  return(page.content)"
      ],
      "metadata": {
        "id": "Bde_I0LWN04d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = get_wikipedia_content(page_title)"
      ],
      "metadata": {
        "id": "yx2AVkvLOZAB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  dir(wikipedia)\n"
      ],
      "metadata": {
        "id": "-v7OYamHlrEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd73feb-ab17-42b2-e3cc-314e3e99e70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['API_URL',\n",
              " 'BeautifulSoup',\n",
              " 'Decimal',\n",
              " 'DisambiguationError',\n",
              " 'HTTPTimeoutError',\n",
              " 'ODD_ERROR_MESSAGE',\n",
              " 'PageError',\n",
              " 'RATE_LIMIT',\n",
              " 'RATE_LIMIT_LAST_CALL',\n",
              " 'RATE_LIMIT_MIN_WAIT',\n",
              " 'RedirectError',\n",
              " 'USER_AGENT',\n",
              " 'WikipediaException',\n",
              " 'WikipediaPage',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'cache',\n",
              " 'datetime',\n",
              " 'debug',\n",
              " 'donate',\n",
              " 'exceptions',\n",
              " 'geosearch',\n",
              " 'languages',\n",
              " 'page',\n",
              " 'random',\n",
              " 're',\n",
              " 'requests',\n",
              " 'search',\n",
              " 'set_lang',\n",
              " 'set_rate_limiting',\n",
              " 'set_user_agent',\n",
              " 'stdout_encode',\n",
              " 'suggest',\n",
              " 'summary',\n",
              " 'sys',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'unicode_literals',\n",
              " 'util',\n",
              " 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgY2FkTdmhTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kw5H5jMlmmS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZF3BiZyXltYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ef7yfa2jl0iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
      ],
      "metadata": {
        "id": "_9aruncMmubX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completions = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a summary assistant at Wikipedia, I will pass you an article and please tell me if any of the information is false\"},\n",
        "        {\"role\": \"user\", \"content\": content[:8180]}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1tWvl0UnTxfE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TMKFGN4nDJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_error_correction(test):\n",
        "    chat_completions = client.chat.completions.create(\n",
        "    model = 'gpt-4',\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'I will be giving you an article. I am looking for false information. I want to capture all potentially false info, if there is even small potential for it to be wrong, please return it. PleaseIf there is no false information only return Done!'},\n",
        "        {'role': 'user', 'content': content[:8180]}]\n",
        "    )\n",
        "    print(chat_completions.choices[0].message.content)"
      ],
      "metadata": {
        "id": "6FKAJVXSoayA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
      ],
      "metadata": {
        "id": "zPw5LyPEobmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles= [\"Artificial Intelligent\", \"UCLA\",\"Rain\"]"
      ],
      "metadata": {
        "id": "V7cuhML2ocGn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page_title in page_titles:\n",
        "  try:\n",
        "    print(\"________________\"+page_title)\n",
        "    content = get_wikipedia_content(page_title)\n",
        "    chatgpt_error_correction(content)\n",
        "  except:\n",
        "    print(\"ERROR\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrwlIoIjRuVo",
        "outputId": "cea23bb7-bd19-492b-a327-3d81b070ae3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________Artificial Intelligent\n",
            "\"Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.\" - Although Alan Turing was a pioneer in theoretical computer science and artificial intelligence, he may not be the first person to conduct substantial research in the field as other researchers such as Claude Shannon, John von Neumann and Warren McCulloch were also significant contributors.\n",
            "\n",
            "\"AIs\" - There is no widely accepted term 'AIs' to refer to Intelligent machines.\n",
            "\n",
            "\"The field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture.\" - The statement oversimplifies the development and progression of the field. Attributing the increase in funding and interest after 2012 solely to the advancement in deep learning and the transformer architecture oversimplifies the various factors contributing to the rise of AI, such as advancements in computational power, data availability, and other algorithmic improvements.\n",
            "\n",
            "\"Even humans rarely use the step-by-step deduction that early AI research could model.\" - It could be challenging to precisely measure or quantify human thought processes in terms of their use of step-by-step deduction. However, humans do employ logical reasoning and deductive reasoning in some cases, which can be sequential and step-by-step. \n",
            "\n",
            "\"AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.\" - While AI indeed draws upon many interdisciplinary fields, the extent and nature of this dependency can vary widely depending on the AI application, task, or research topic at hand. This statement fails to clarify that nuance.\n",
            "\n",
            "\"A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way, and a reward function that supplies the utility of each state and the cost of each action.\" - While this statement is generally accurate, it may oversimplify the complexities and assumptions of a Markov Decision Process (MDP), such as the implications of a stationary policy and perfect state observability. \n",
            "\n",
            "\"Game theory describes rational behavior of multiple interacting agents, and is used in AI programs that make decisions that involve other agents.\" - While game theory can be used in the design and analysis of multi-agent AI systems, how and to what extent it is used can significantly vary depending on the specific context and problem domain of the AI system.\n",
            "________________UCLA\n",
            "1. \"The University of California, Los Angeles (UCLA) is a public land-grant research university in Los Angeles, California, United States.\" - UCLA is a public university, but it is not a land-grant institution.\n",
            "2. \"Its academic roots were established in 1881 as a normal school then known as the southern branch of the California State Normal School which later evolved into San José State University.\" - UCLA's real predecessor was the southern branch of the California State Normal School, but this did not evolve into San José State University. Instead, it became UCLA.\n",
            "3. \"UCLA offers 337 undergraduate and graduate degree programs\" - UCLA actually offers more than 125 undergraduate majors and 150 graduate degree programs.\n",
            "4. \"It received 174,914 undergraduate applications for Fall 2022,\" - This seems like an estimate as we don't have the exact figures released for Fall 2022.\n",
            "5. \"They have won 121 NCAA team championships, second only to Stanford University's 128 team titles.\" - As of 2022, UCLA has won 118 NCAA championships, not 121.\n",
            "6. \"Six of the schools offer undergraduate degree programs: Arts and Architecture, Engineering and Applied Science, Music, Nursing, Public Affairs, and Theater, Film and Television.\" - UCLA offers undergraduate degree programs in seven schools, not six, these are College of Letters and Science, the School of the Arts and Architecture, the School of Engineering and Applied Science, the School of Music, the School of Nursing, the School of Theater, Film & Television, and the Luskin School of Public Affairs.\n",
            "7. \"The university was elected to the Association of American Universities in 1974.\" - UCLA became a member of the Association of American Universities in 1972, not 1974.\n",
            "8. \"On May 23, 1919, the Southern Californians' efforts were rewarded when Governor William D. Stephens signed Assembly Bill 626 into law, which acquired the land and buildings and transformed the Los Angeles Normal School into the Southern Branch of the University of California.\" - The bill was signed into law on May 23, 1919, but it was Senate Bill 211, not Assembly Bill 626.\n",
            "9. \"The original four buildings were the College Library (now Powell Library), Royce Hall, the Physics-Biology Building (which became the Humanities Building and is now the Renee and David Kaplan Hall), and the Chemistry Building (now Haines Hall),\" - The original four buildings were the Biological and Chemical Sciences Building (today’s Haines Hall), the Quantum Physics Building (today’s Kaplan Hall), Royce Hall, and the Library (today’s Powell Library).\n",
            "10. \"The Regents announced the new \"Beverly Site\" — just west of Beverly Hills — in 1925.\" - The Future Site of the Southern Branch of the University of California — just west of Beverly Hills — was announced in 1926, not 1925.\n",
            "11. \"In 1951, UCLA was formally elevated to co-equal status with UC Berkeley, and its presiding officer Raymond B. Allen was the first chief executive to be granted the title of chancellor.\" - UCLA’s first chief executive officer to be granted the title of chancellor was Raymond B. Allen in 1952, not 1951.\n",
            "________________Rain\n",
            "1. \"Trains have their roots in wagonways, which used railway tracks and were powered by horses or pulled by cables.\" - Some early wagonways used a combination of horse power and gravity, not cables.\n",
            "2. \"Following the invention of the steam locomotive in the United Kingdom in 1802\" - The first full-scale working railway steam locomotive was built by George Stephenson, an English engineer, in 1814, not 1802.\n",
            "3. \"Trains are an evolution of wheeled wagons running on stone wagonways, the earliest of which were built by Babylon circa 2,200 BCE.\" - There might be evidence for wooden railways from this period but not stone wagonways.\n",
            "4. \"In 1804 a steam locomotive built by British inventor Richard Trevithick powered the first ever steam train.\" - The first steam-locomotive powered train was by George Stephenson in 1814, not Trevithick in 1804.\n",
            "5. \"British engineer George Stephenson ran a steam locomotive named Locomotion No. 1 on this 40-kilometer (25-mile) long line, hauling over 400 passengers at up to 13 kilometers per hour (8 mph).\" - While George Stephenson was a central figure in early rail transport, he did not personally run the Locomotion No. 1. This was done by his associate, Timothy Hackworth.\n",
            "6. \"China was the last country to fully dieselize, due to its abundant coal reserves; steam locomotives were used to haul mainline trains as late as 2005 in Inner Mongolia.\" - While it's accurate that China was among the last countries to phase out steam locomotives, there have been occasional reports of steam locomotives still in occasional operation beyond 2005.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kza1P4D4RuXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}