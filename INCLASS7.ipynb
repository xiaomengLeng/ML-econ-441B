{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKSTLF2BX6jH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11Ee3GJmywu",
        "outputId": "c92003c2-8981-4da7-b749-b113d66cfd4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=32baa406a743c911cda100b54f49de60f419ab19d31b895bc32f5533220119f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: h11, wikipedia, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0 wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import wikipedia"
      ],
      "metadata": {
        "id": "Q2A8TGhKm3i5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.) Set up OpenAI and the enviornment\n"
      ],
      "metadata": {
        "id": "7E9HEMJSX-3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apikey = \"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\""
      ],
      "metadata": {
        "id": "4zwwdkZDYDZN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client= openai.OpenAI(\n",
        "    api_key=apikey\n",
        ")"
      ],
      "metadata": {
        "id": "8IiKS0snlpYP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSL--dhXMvnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.) Use the wikipedia api to get a function that pulls in the text of a wikipedia page"
      ],
      "metadata": {
        "id": "tOXc5_BTm9HP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles= [\"Artificial\", \"Intelligent\", \"UCLA\"]"
      ],
      "metadata": {
        "id": "jFEF-h6tNJgh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_title =page_titles[0]"
      ],
      "metadata": {
        "id": "ZEBlMc-VNZmk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_result =wikipedia.search(page_title)"
      ],
      "metadata": {
        "id": "upFPbkr3Nd7D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = wikipedia.search(search_result[0])"
      ],
      "metadata": {
        "id": "nz0bYaIsNuqP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wikipedia_content(page_title):\n",
        "  search_result =wikipedia.search(page_title)\n",
        "  page = wikipedia.page(search_result[0])\n",
        "  return(page.content)"
      ],
      "metadata": {
        "id": "Bde_I0LWN04d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = get_wikipedia_content(page_title)"
      ],
      "metadata": {
        "id": "yx2AVkvLOZAB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  dir(wikipedia)\n"
      ],
      "metadata": {
        "id": "-v7OYamHlrEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4790b9-076c-46ac-8ef3-bb05b80d8404"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['API_URL',\n",
              " 'BeautifulSoup',\n",
              " 'Decimal',\n",
              " 'DisambiguationError',\n",
              " 'HTTPTimeoutError',\n",
              " 'ODD_ERROR_MESSAGE',\n",
              " 'PageError',\n",
              " 'RATE_LIMIT',\n",
              " 'RATE_LIMIT_LAST_CALL',\n",
              " 'RATE_LIMIT_MIN_WAIT',\n",
              " 'RedirectError',\n",
              " 'USER_AGENT',\n",
              " 'WikipediaException',\n",
              " 'WikipediaPage',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " 'cache',\n",
              " 'datetime',\n",
              " 'debug',\n",
              " 'donate',\n",
              " 'exceptions',\n",
              " 'geosearch',\n",
              " 'languages',\n",
              " 'page',\n",
              " 'random',\n",
              " 're',\n",
              " 'requests',\n",
              " 'search',\n",
              " 'set_lang',\n",
              " 'set_rate_limiting',\n",
              " 'set_user_agent',\n",
              " 'stdout_encode',\n",
              " 'suggest',\n",
              " 'summary',\n",
              " 'sys',\n",
              " 'time',\n",
              " 'timedelta',\n",
              " 'unicode_literals',\n",
              " 'util',\n",
              " 'wikipedia']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgY2FkTdmhTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kw5H5jMlmmS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZF3BiZyXltYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ef7yfa2jl0iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.) Build a chatgpt bot that will analyze the text given and try to locate any false info"
      ],
      "metadata": {
        "id": "_9aruncMmubX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completions = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a summary assistant at Wikipedia, I will pass you an article and please tell me if any of the information is false\"},\n",
        "        {\"role\": \"user\", \"content\": content[:8180]}\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1tWvl0UnTxfE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completions.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_TMKFGN4nDJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56316158-2d93-490c-f784-d9bc9c34f725"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The information in the article is true. Artificiality refers to the state of being human-made or unnatural. While it may carry negative connotations of being false or deceptive, it can also demonstrate human capacity to replicate nature, as in cases like artificial hearts or artificial intelligence. Distinguishing between natural and artificial objects is generally possible, with artificial environments exhibiting more physical regularity. The philosophical arguments over the definitions and implications of artificiality are also accurately represented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_error_correction(text):\n",
        "    chat_completions = client.chat.completions.create(\n",
        "    model = 'gpt-4',\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'I will be giving you an article. I am looking for false information. I want to capture all potentially false info, if there is even small potential for it to be wrong, please return it. PleaseIf there is no false information only return Done!'},\n",
        "        {'role': 'user', 'content': content[:8180]}]\n",
        "    )\n",
        "    print(chat_completions.choices[0].message.content)"
      ],
      "metadata": {
        "id": "6FKAJVXSoayA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.) Make a for loop and check a few wikipedia pages and return a report of any potentially false info via wikipedia"
      ],
      "metadata": {
        "id": "zPw5LyPEobmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_titles= [\"Artificial Intelligent\", \"UCLA\",\"Rain\"]"
      ],
      "metadata": {
        "id": "V7cuhML2ocGn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for page_title in page_titles:\n",
        "  try:\n",
        "    print(\"________________\"+page_title)\n",
        "    content = get_wikipedia_content(page_title)\n",
        "    chatgpt_error_correction(content)\n",
        "  except:\n",
        "    print(\"ERROR\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrwlIoIjRuVo",
        "outputId": "7211859e-7b95-42ed-98d1-025fc765dba1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________Artificial Intelligent\n",
            "\"Alan Turing was the first person to conduct substantial research in the field that he called machine intelligence.\" - While Alan Turing was a very influential figure in the foundations of artificial intelligence, it might not be accurate to say he was the first person to conduct substantial research in the field. Other notable figures, like John McCarthy, Marvin Minsky, Allen Newell, and Herbert A. Simon, also made significant contributions around the same time.\n",
            "\"Artificial intelligence was founded as an academic discipline in 1956.\" - While the Dartmouth Conference in 1956 is often considered the birth of AI as a field of study, earlier work done by individuals such as Alan Turing and others predates this.\n",
            "\"Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI spring of the early 2020s\"- This statement could be misleading; AI funding and interest have had multiple rises and falls, not just after 2012 and 2017. The so-called \"AI spring\" is a period of renewed interest and funding, it's not strictly tied to those dates or events.\n",
            "\"General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.\" - This definition of general intelligence could be misleading, as it's not universally agreed upon in the field.\n",
            "\"Among the most difficult problems in knowledge representation are: the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous) ...\" - The phrase \"atomic facts\" isn't commonly used in discussions about common sense knowledge.\n",
            "\"In classical planning, the agent knows exactly what the effect of any action will be.\" - This statement might be false or misleading, as the outcomes of actions in any environment, even in classical planning, can be influenced by a variety of factors and may not be exactly predictable.\n",
            "\"Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\" - While machine learning techniques are an integral part of modern AI, it wasn't always a key component of AI. Early AI systems were largely rule-based and didn't necessarily incorporate machine learning techniques.\n",
            "________________UCLA\n",
            "\"UCLA offers 337 undergraduate and graduate degree programs\"\n",
            "\"SIX of the schools offer undergraduate degree programs: Arts and Architecture, Engineering and Applied Science, Music, Nursing, Public Affairs, and Theater, Film and Television.\"\n",
            "\"410 Bruins have made Olympic teams, winning 270 Olympic medals: 136 gold, 71 silver and 63 bronze.\"\n",
            "\"UCLA has been represented in every Olympics since the university's founding (except in 1924) and has had a gold medalist in every Olympics in which the U.S. has participated since 1932.\"\n",
            "\"As of October 2021, 27 Nobel laureates, five Turing Award winners, two Chief Scientists of the U.S. Air Force and one Fields Medalist have been affiliated with it as faculty, researchers and alumni.\"\n",
            "\"As of August 2021, 55 associated faculty members have been elected to the National Academy of Sciences, 29 to the National Academy of Engineering, 41 to the National Academy of Medicine and 156 to the American Academy of Arts and Sciences.\"\n",
            "\"They have won 121 NCAA team championships, second only to Stanford University's 128 team titles.\"\n",
            "\"The Regents announced the new \"Beverly Site\" — just west of Beverly Hills — in 1925.\"\n",
            "\"On February 1, 1927, the Regents renamed the Southern Branch the University of California at Los Angeles.\"\n",
            "\"In 1933, UCLA was permitted to award the master's degree and the doctorate in 1936, against continued resistance from UC Berkeley.\"\n",
            "\"In 1951, UCLA was formally elevated to co-equal status with UC Berkeley, and its presiding officer Raymond B. Allen was the first chief executive to be granted the title of chancellor.\"\n",
            "\"On June 1, 2016, two men were killed in a murder-suicide at an engineering building in the university.\"\n",
            "\"In 2018, a student-led community coalition known as \"Westwood Forward\" successfully led an effort to break UCLA and Westwood Village away from the existing Westwood Neighborhood Council and form a new North Westwood Neighborhood Council.\"\n",
            "\"In 2022, UCLA signed an agreement to partner with the Tongva for the caretaking and landscaping of various areas of the campus.\"\n",
            "________________Rain\n",
            "1. The text mentions \"Following the invention of the steam locomotive in the United Kingdom in 1802\". This could potentially be false information as the first full-scale working railway steam locomotive was built by George Stephenson in 1814, not in 1802.\n",
            "2. The text says that the \"first ever steam train\" was powered by a steam locomotive built by British inventor Richard Trevithick in 1804. However, it is generally accepted that George Stephenson built the first full-scale working railway steam locomotive in 1814.\n",
            "3. The text says \"Trains began to face strong competition from automobiles and freight trucks in the 1930s\". This could potentially be incorrect as cars and trucks were not commonly used for long-distance travel and transportation until after World War II - in the 1940s and 1950s.\n",
            "4. The article states \"China was the last country to fully dieselize, due to its abundant coal reserves; steam locomotives were used to haul mainline trains as late as 2005 in Inner Mongolia\". This could potentially be false as steam locomotives continued to be used in some areas of the world beyond 2005. For example, in Zimbabwe and India, steam locomotives were reportedly still in use as of 2010.\n",
            "5. The claim \"Trains are an evolution of wheeled wagons running on stone wagonways, the earliest of which were built by Babylon circa 2,200 BCE\" is potentially false. The earliest documented use of stone wagonways is in ancient Greece, not Babylon.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kza1P4D4RuXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}